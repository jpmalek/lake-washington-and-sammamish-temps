{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebe631fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 08:26:22,526 - INFO - Chrome WebDriver quit successfully.\n",
      "2024-03-25 08:26:22,593 - INFO - Chrome WebDriver quit successfully.\n",
      "2024-03-25 08:26:22,775 - INFO - Download directory: /Users/jeffmalek/Documents/git/lake_washington_and_sammamish_temps/downloads\n",
      "2024-03-25 08:26:22,776 - INFO - New file base: /Users/jeffmalek/Documents/git/lake_washington_and_sammamish_temps/downloads\n",
      "2024-03-25 08:26:22,777 - INFO - Download directory: /Users/jeffmalek/Documents/git/lake_washington_and_sammamish_temps/downloads\n",
      "2024-03-25 08:26:22,846 - INFO - New file base: \n",
      "2024-03-25 08:26:23,306 - INFO - S3 bucket 'lake-water-data' already exists.\n",
      "2024-03-25 08:26:23,829 - INFO - Getting latest production json for highs and lows from https://swimming.withelvis.com/WA/lake_wa_highs_and_lows.json with 1440 days to cache.\n",
      "2024-03-25 08:26:24,032 - INFO - Response server time: Mon, 25 Mar 2024 14:52:03 GMT Last modified: Thu, 07 Mar 2024 06:41:19 GMT Age in seconds: 2062\n",
      "2024-03-25 08:26:24,034 - INFO - Data for highs and lows Last updated: 2024-03-06 22:41:18 time diff: 18 days, 8:45:06.034154 hours diff: 440.75167615388887 hours to cache: 1440\n",
      "2024-03-25 08:26:24,034 - INFO - Getting latest production json for lake temps from https://swimming.withelvis.com/WA/lake_temps.json with 8 days to cache.\n",
      "2024-03-25 08:26:24,140 - INFO - Response server time: Mon, 25 Mar 2024 15:23:15 GMT Last modified: Mon, 18 Mar 2024 14:52:11 GMT Age in seconds: 190\n",
      "2024-03-25 08:26:24,141 - INFO - Data for lake temps Last updated: 2024-03-18 07:52:10 time diff: 7 days, 0:34:14.141369 hours diff: 168.5705948247222 hours to cache: 8\n",
      "2024-03-25 08:26:24,144 - INFO - prod_url_dict: {\n",
      "    \"highs and lows\": {\n",
      "        \"url\": \"https://swimming.withelvis.com/WA/lake_wa_highs_and_lows.json\",\n",
      "        \"hours_to_cache\": 1440,\n",
      "        \"update\": false\n",
      "    },\n",
      "    \"lake temps\": {\n",
      "        \"url\": \"https://swimming.withelvis.com/WA/lake_temps.json\",\n",
      "        \"hours_to_cache\": 8,\n",
      "        \"update\": true\n",
      "    }\n",
      "}\n",
      "2024-03-25 08:26:24,144 - INFO - +++++++++ Not time to process all historical data.\n",
      "2024-03-25 08:26:24,145 - INFO - +++++++++ Processing real-time lake data...\n",
      "2024-03-25 08:26:24,145 - INFO - Getting realtime lake data from https://green2.kingcounty.gov/lake-buoy/GenerateMapData.aspx\n",
      "2024-03-25 08:26:24,725 - INFO - Got realtime lake data from https://green2.kingcounty.gov/lake-buoy/GenerateMapData.aspx\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 830\u001b[0m\n\u001b[1;32m    827\u001b[0m     king_county_lakes\u001b[38;5;241m.\u001b[39mgo()\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 830\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[6], line 827\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m    826\u001b[0m     king_county_lakes \u001b[38;5;241m=\u001b[39m KingCountyLakes()\n\u001b[0;32m--> 827\u001b[0m     king_county_lakes\u001b[38;5;241m.\u001b[39mgo()\n",
      "Cell \u001b[0;32mIn[6], line 816\u001b[0m, in \u001b[0;36mKingCountyLakes.go\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprod_url_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlake temps\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_manager\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+++++++++ Processing real-time lake data...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 816\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_realtime_lake_data()\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvalidate_cloudfront_cache()\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[6], line 765\u001b[0m, in \u001b[0;36mKingCountyLakes.get_realtime_lake_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    763\u001b[0m fields \u001b[38;5;241m=\u001b[39m segment\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    764\u001b[0m lake_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlake_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfields[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 765\u001b[0m temperature_celsius \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(fields[\u001b[38;5;241m6\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_manager\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msegment\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m temp: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemperature_celsius\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    767\u001b[0m temperature_fahrenheit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_processor\u001b[38;5;241m.\u001b[39mcelsius_to_fahrenheit(\u001b[38;5;28mfloat\u001b[39m(temperature_celsius))\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: ''"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from botocore.exceptions import ClientError\n",
    "from chromedriver_py import binary_path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import pytz\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\"\"\"\n",
    "    See README for more information on this app.\n",
    "\"\"\"         \n",
    "class FileManager:\n",
    "    \"\"\"\n",
    "    A class for managing file operations on either a local file system or AWS S3, including\n",
    "    backing up, moving files, and ensuring an S3 bucket exists.\n",
    "    \"\"\"\n",
    "    def __init__(self, use_s3=False, s3_bucket='lake-water-data', aws_region='us-west-2'):\n",
    "        \"\"\"\n",
    "        Initializes the FileManager with optional S3 support. If S3 is used, checks for the\n",
    "        existence of the specified S3 bucket and creates it if it doesn't exist.\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger()\n",
    "        self.download_file_extension = '.csv'\n",
    "        self.backup_file_extension = 'backup'\n",
    "        self.downloads_folder = 'downloads'\n",
    "        self.min_file_size = 5000\n",
    "\n",
    "        self.header_written = False\n",
    "\n",
    "        self.download_directory = os.path.join(os.getcwd(), self.downloads_folder)\n",
    "        self.logger.info(f'Download directory: {self.download_directory}')\n",
    "        self.clean_download_directory(self.download_directory)   \n",
    "\n",
    "        self.use_s3 = use_s3\n",
    "        self.s3_bucket = s3_bucket\n",
    "        self.aws_region = aws_region\n",
    "        self.s3_client = boto3.client('s3', region_name=self.aws_region) if use_s3 else \"\"\n",
    "\n",
    "        self.new_file_base = self.download_directory if not self.use_s3 else \"\" \n",
    "        self.logger.info(f'New file base: {self.new_file_base }')\n",
    "        # Check and create the S3 bucket if necessary\n",
    "        if self.use_s3:\n",
    "            self.ensure_bucket_exists()\n",
    "\n",
    "    def backup_file(self, old_file, new_file):\n",
    "            \"\"\"\n",
    "            Backs up a file either locally or on AWS S3.\n",
    "\n",
    "            Args:\n",
    "                old_file (str): The path to the file to back up.\n",
    "                new_file (str): The path or key to the backup file.\n",
    "\n",
    "            Raises:\n",
    "                FileNotFoundError: If the old file does not exist.\n",
    "                Exception: For errors encountered when using AWS S3.\n",
    "            \"\"\"\n",
    "            if self.use_s3:\n",
    "                try:\n",
    "                    self.logger.info(f'Backing up file to S3: {old_file} -> {new_file}')\n",
    "                    self.s3_client.copy_object(Bucket=self.s3_bucket, CopySource={'Bucket': self.s3_bucket, 'Key': old_file}, Key=new_file)\n",
    "                    # Delete the original file\n",
    "                    self.logger.info(f'Deleting old file in S3: {old_file}')\n",
    "                    self.s3_client.delete_object(Bucket=self.s3_bucket, Key=old_file)\n",
    "                    \n",
    "                except FileNotFoundError:\n",
    "                    self.logger.info(f\"The file {old_file} does not exist.\")\n",
    "                except ClientError as e:\n",
    "                    self.logger.info(f'Failed to back up file to S3: {e}')\n",
    "            else:\n",
    "                if os.path.isfile(old_file):\n",
    "                    self.logger.info(f'Backing up old file locally: {old_file} -> {new_file}')\n",
    "                    os.rename(old_file, new_file)\n",
    "\n",
    "    def move_file(self, source, destination,remove_source=True):\n",
    "        \"\"\"\n",
    "        Moves a file either locally or on AWS S3.\n",
    "\n",
    "        Args:\n",
    "            source (str): The path or key of the source file.\n",
    "            destination (str): The path or key of the destination file.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the source file does not exist (for local moves).\n",
    "            Exception: For errors encountered when using AWS S3.\n",
    "        \"\"\"\n",
    "        if self.use_s3:\n",
    "            try:\n",
    "                self.logger.info(f'Moving file from local to S3: {source} -> {destination}')\n",
    "                # Upload the file to the new location on S3\n",
    "                content_type = 'application/json' if source.endswith('.json') else 'text/csv'\n",
    "                self.s3_client.upload_file(source, self.s3_bucket, destination, ExtraArgs={'ContentType': content_type})\n",
    "                # If upload was successful, delete the local file\n",
    "                if remove_source == True:\n",
    "                    os.remove(source)\n",
    "                self.logger.info(f'Successfully moved {source} to {destination} on S3.')\n",
    "            except ClientError as e:\n",
    "                self.logger.error(f'Failed to move file to S3: {e}')\n",
    "            except FileNotFoundError:\n",
    "                self.logger.error(f'Local file not found: {source}')\n",
    "        else:\n",
    "            if os.path.isfile(source):\n",
    "                self.logger.info(f'Moving file locally: {source} -> {destination}')\n",
    "                os.rename(source, destination)\n",
    "            else:\n",
    "                self.logger.error(f'File not found: {source}')\n",
    "                raise FileNotFoundError(f'File not found: {source}')\n",
    "\n",
    "    def ensure_bucket_exists(self):\n",
    "        \"\"\"\n",
    "        Checks for the existence of the S3 bucket by name and creates it if it doesn't exist.\n",
    "        \"\"\"\n",
    "        if not self.s3_bucket:\n",
    "            self.logger.error(\"No S3 bucket name provided.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Check if the bucket exists and create it if not\n",
    "            existing_buckets = self.s3_client.list_buckets().get('Buckets', [])\n",
    "            bucket_names = [bucket['Name'] for bucket in existing_buckets]\n",
    "            if self.s3_bucket in bucket_names:\n",
    "                self.logger.info(f\"S3 bucket '{self.s3_bucket}' already exists.\")\n",
    "            else:\n",
    "                # Create the bucket\n",
    "                self.s3_client.create_bucket(Bucket=self.s3_bucket,CreateBucketConfiguration={'LocationConstraint': self.aws_region})\n",
    "                self.logger.info(f\"S3 bucket '{self.s3_bucket}' created successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to check or create S3 bucket: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def clean_download_directory(self,download_directory):\n",
    "        if os.path.isdir(download_directory):\n",
    "            for f in os.listdir(download_directory):\n",
    "                if f.endswith('.crdownload') or f.endswith('.csv') or f.endswith('.txt'):\n",
    "                    path = os.path.join(download_directory, f)\n",
    "                    self.logger.info(f'Removing file for cleanup:{path}')\n",
    "                    os.remove(path)\n",
    "    \n",
    "    def wait_for_download_to_complete(self,timeout=300):\n",
    "        end_time = time.time() + timeout\n",
    "        while True:\n",
    "            # Check for any files not ending in '.crdownload'\n",
    "            if not any(f.endswith('.crdownload') for f in os.listdir(self.download_directory)):\n",
    "                # Assuming there's only one file being downloaded or you know the filename\n",
    "                files = [f for f in os.listdir(self.download_directory) if (f.endswith(self.download_file_extension) or f.endswith('.txt'))]\n",
    "                if files:\n",
    "                    # Check if file size has stabilized indicating download completion\n",
    "                    filepath = os.path.join(self.download_directory, files[0])\n",
    "                    size = os.path.getsize(filepath)\n",
    "                    self.logger.info(f'Size of {filepath} : {size}')\n",
    "                    time.sleep(2)  # Wait for 2 seconds to see if the file size changes\n",
    "                    if size == os.path.getsize(filepath) and size > self.min_file_size:\n",
    "                        self.logger.info(f'Size of {filepath} : {size}')\n",
    "                        self.logger.info(f'Download completed: {filepath}')\n",
    "                        return filepath\n",
    "            if time.time() > end_time:\n",
    "                raise Exception(\"Timeout: File download did not complete in the allotted time.\")\n",
    "            time.sleep(1)  # Polling interval\n",
    "\n",
    "    def mkdirs(self, path):\n",
    "        \"\"\"\n",
    "        Creates a folder structure either in S3 or locally, based on the use_s3 flag.\n",
    "        \n",
    "        Parameters:\n",
    "            path (str): The path or key to create. For S3, it should be the key structure.\n",
    "                        For local, it's the directory path.\n",
    "        \"\"\"\n",
    "        if self.use_s3:\n",
    "            self._create_s3_directory_structure(path)\n",
    "        else:\n",
    "            self._create_local_directory_structure(path)\n",
    "    \n",
    "    def delete_first_n_lines(self,file_path, lines_to_delete=12, output_file_path=None):\n",
    "        \"\"\"Delete the first n lines from a file.\n",
    "\n",
    "        Args:\n",
    "        - file_path: Path to the input file.\n",
    "        - n: Number of lines to delete from the beginning of the file.\n",
    "        - output_file_path: Path to the output file. If None, the input file is overwritten.\n",
    "\n",
    "        This function reads the input file, skips the first n lines, and writes the rest\n",
    "        to the output file. If an output file is not specified, it overwrites the input file.\n",
    "        \"\"\"\n",
    "        self.logger.info(f'Deleting first {lines_to_delete} lines from {file_path}')    \n",
    "        if lines_to_delete < 2:\n",
    "            return\n",
    "        \n",
    "        # If no output file path is provided, overwrite the input file\n",
    "        if output_file_path is None:\n",
    "            output_file_path = file_path\n",
    "        \n",
    "        with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        with open(output_file_path, 'w', encoding='ISO-8859-1') as file:\n",
    "            file.writelines(lines[lines_to_delete:])\n",
    "\n",
    "    def _create_s3_directory_structure(self, path):\n",
    "        \"\"\"\n",
    "        Simulates creating a directory structure in S3 by creating an empty object with a key\n",
    "        that ends in a slash (/).\n",
    "        \n",
    "        Parameters:\n",
    "            path (str): The S3 key structure to create.\n",
    "        \"\"\"\n",
    "        # Ensure the path ends with a slash\n",
    "        if not path.endswith('/'):\n",
    "            path += '/'\n",
    "        try:\n",
    "            self.s3_client.put_object(Bucket=self.s3_bucket, Key=path)\n",
    "            self.logger.info(f\"Created S3 directory structure: {path} in bucket: {self.s3_bucket}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating S3 directory structure: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _create_local_directory_structure(self, path):\n",
    "        \"\"\"\n",
    "        Creates a directory structure locally using os.makedirs.\n",
    "        \n",
    "        Parameters:\n",
    "            path (str): The local directory path to create.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            self.logger.info(f\"Created local directory structure: {path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating local directory structure: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def combine_csv(self, input_file,output_file):\n",
    "        \"\"\"\n",
    "        Processes an input CSV file, discarding the first 12 lines of non-CSV data, and appends\n",
    "        the remaining data to the output file. Prepends lake name and monitoring site name to each row.\n",
    "\n",
    "        Parameters:\n",
    "        - input_file (str): The path to the input CSV file.\n",
    "        - output_file (str): The path to the output CSV file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract lake name and monitoring site name from the file name\n",
    "            lake_name, monitoring_site = os.path.basename(input_file).split('.')[0].split('-')\n",
    "            self.logger.info(f\"Processing {input_file} for lake {lake_name} and monitoring site {monitoring_site} to output file {output_file}\")\n",
    "            with open(input_file, 'r', encoding='ISO-8859-1') as infile:\n",
    "                # Skip the first 12 lines of non-CSV data\n",
    "                for _ in range(12):\n",
    "                    next(infile)\n",
    "                csv_reader = csv.reader(infile)\n",
    "                header = next(csv_reader)  # Read the header\n",
    "\n",
    "                with open(output_file, 'a', newline='', encoding='ISO-8859-1') as outfile:\n",
    "                    csv_writer = csv.writer(outfile)\n",
    "                    \n",
    "                    # Write the header if it hasn't been written yet\n",
    "                    if not self.header_written:\n",
    "                        csv_writer.writerow(['LakeName', 'MonitoringSite'] + header)\n",
    "                        self.header_written = True\n",
    "                    \n",
    "                    # Write each row of data, prepending lake name and monitoring site\n",
    "                    for row in csv_reader:\n",
    "                        csv_writer.writerow([lake_name, monitoring_site] + row)\n",
    "                        \n",
    "            self.logger.info(f\"Processed and combined CSV file: {input_file} > {output_file}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing file {input_file}: {e}\")\n",
    "    \n",
    "class ChromeUtility:\n",
    "    \"\"\"\n",
    "    A class for managing Chrome WebDriver operations, including setting up the WebDriver.\n",
    "    \"\"\"\n",
    "    def __init__(self,headless=True,download_directory='downloads'):\n",
    "        self.logger = logging.getLogger()\n",
    "        self.binary_path = binary_path\n",
    "        self.pacific_tz = pytz.timezone('America/Los_Angeles')\n",
    "        self.historical_locations = '' \n",
    "\n",
    "        self.options = Options()\n",
    "        self.options.add_argument(\"--headless\") if headless else self.options.add_argument(\"--start-maximized\")\n",
    "        self.options.add_argument(\"--disable-gpu\")  # Disables GPU hardware acceleration. If software renderer is not in place, then the GPU process won't launch.\n",
    "        self.options.add_argument(\"--no-sandbox\")  # Bypasses OS security model; recommended for running in Docker or CI/CD environments.\n",
    "        self.options.add_argument(\"--disable-dev-shm-usage\")  # Overcomes limited resource problems.\n",
    "        self.options.add_experimental_option(\"prefs\", {\n",
    "            \"download.default_directory\": download_directory,\n",
    "            \"download.prompt_for_download\": False,  # Disables prompt\n",
    "            \"download.directory_upgrade\": True,\n",
    "            \"safebrowsing.enabled\": True  # Disables safebrowsing to allow automatic file downloads\n",
    "            })\n",
    "        self.service = Service(executable_path=binary_path)\n",
    "        # Setup WebDriver\n",
    "        self.driver = webdriver.Chrome(service=self.service,options=self.options)\n",
    "    \n",
    "    def get_historical_wa_lake_data_locations(self):\n",
    "        \"\"\"\n",
    "        Get historical lake temperature locations from the Washington State Department of Ecology.\n",
    "        \"\"\"\n",
    "        self.driver.get('https://green2.kingcounty.gov/lakes/Query.aspx')\n",
    "        self.logger.info(f'Navigated to {self.driver.title}')\n",
    "        # Find the dropdown element by its ID\n",
    "        dropdown = Select(self.driver.find_element(By.ID, \"ctl00_kcMasterPagePlaceHolder_LocatorDropDownList\"))\n",
    "        # List comprehension to get the text of each option\n",
    "        locations = [option.text for option in dropdown.options[1:]]\n",
    "        #json_config = json.dumps(locations,indent=4)\n",
    "        self.historical_locations = locations\n",
    "        return locations\n",
    "    \n",
    "    def get_historical_wa_lake_data(self,start_date='01/01/1983',end_date= datetime.now().strftime('%m/%d/%Y'),just_get_temps=True,location=None):\n",
    "        \"\"\"\n",
    "        Get historical lake temperature data from the Washington State Department of Ecology.\n",
    "        \"\"\"\n",
    "        end_date = datetime.now(self.pacific_tz).strftime('%m/%d/%Y')\n",
    "\n",
    "        # Page reloads upon select. Re-locate the dropdown before interacting with it to avoid StaleElementReferenceException\n",
    "        dropdown_element = self.driver.find_element(By.ID, \"ctl00_kcMasterPagePlaceHolder_LocatorDropDownList\")\n",
    "        dropdown = Select(dropdown_element)\n",
    "        # Now, select the next option\n",
    "        dropdown.select_by_visible_text(location)\n",
    "        self.logger.info(f'Option text: {location}')\n",
    "        WebDriverWait(self.driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"ctl00_kcMasterPagePlaceHolder_StartDateTextBox\"))\n",
    "        )\n",
    "        # Fill out the form (update these selectors based on actual form structure)\n",
    "        self.driver.find_element(By.ID, 'ctl00_kcMasterPagePlaceHolder_StartDateTextBox').clear()\n",
    "        self.driver.find_element(By.ID, 'ctl00_kcMasterPagePlaceHolder_StartDateTextBox').send_keys(start_date)\n",
    "        self.driver.find_element(By.ID, 'ctl00_kcMasterPagePlaceHolder_EndDateTextBox').clear()\n",
    "        self.driver.find_element(By.ID, 'ctl00_kcMasterPagePlaceHolder_EndDateTextBox').send_keys(end_date)\n",
    "        if just_get_temps:\n",
    "            if not self.driver.find_element(By.ID, 'ctl00_kcMasterPagePlaceHolder_TEMP').is_selected():\n",
    "                self.driver.find_element(By.ID, 'ctl00_kcMasterPagePlaceHolder_TEMP').click()\n",
    "        else:\n",
    "            self.driver.find_element(By.ID, 'Checkboxes').click()\n",
    "        # Submit the form\n",
    "        self.driver.find_element(By.ID, 'ctl00_kcMasterPagePlaceHolder_DownLoadButton').click()\n",
    "    \n",
    "    def get_cached_realtime_wa_lake_data_locations(self):\n",
    "        \"\"\"\n",
    "        Get realtime lake temperature locations from the Washington State Department of Ecology.\n",
    "        \"\"\"\n",
    "        self.driver.get('https://green2.kingcounty.gov/lake-buoy/Data.aspx')\n",
    "        self.logger.info(f'Navigated to {self.driver.title}')\n",
    "        # Find the dropdown element by its ID\n",
    "        data_type_dropdown = Select(self.driver.find_element(By.ID, \"ctl00_kcMasterPagePlaceHolder_c_TypeDropDownList\"))\n",
    "        # List comprehension to get the text of each option\n",
    "        data_types = [option.text for option in data_type_dropdown.options[1:]]\n",
    "        data_type_dropdown.select_by_visible_text(data_types[0])\n",
    "        current_year = datetime.now(self.pacific_tz).year.__str__()\n",
    "        # Wait for the specific value to be available in the drop-down list\n",
    "        WebDriverWait(self.driver, 10).until(\n",
    "            EC.text_to_be_present_in_element_value((By.ID, 'ctl00_kcMasterPagePlaceHolder_c_YearDropDownList'), current_year)\n",
    "        )\n",
    "        # Find the dropdown element by its ID\n",
    "        location_dropdown = Select(self.driver.find_element(By.ID, \"ctl00_kcMasterPagePlaceHolder_c_BuoyDropDownList\"))\n",
    "        # List comprehension to get the text of each option\n",
    "        locations = [option.text for option in location_dropdown.options if 'inactive' not in option.text.lower()]\n",
    "        return locations\n",
    "\n",
    "        \n",
    "    \n",
    "    def get_realtime_wa_lake_data(self,location=None):\n",
    "        \"\"\"\n",
    "        Get historical lake temperature data from the Washington State Department of Ecology.\n",
    "        \"\"\"\n",
    "        # Page reloads upon select. Re-locate the dropdown before interacting with it to avoid StaleElementReferenceException\n",
    "        dropdown_element = self.driver.find_element(By.ID, \"ctl00_kcMasterPagePlaceHolder_c_BuoyDropDownList\")\n",
    "        dropdown = Select(dropdown_element)\n",
    "        # Now, select the next option\n",
    "        dropdown.select_by_visible_text(location)\n",
    "        self.logger.info(f'Option text: {location}')\n",
    "        current_year = datetime.now(self.pacific_tz).year.__str__()\n",
    "        current_month = datetime.now(self.pacific_tz).month.__str__()\n",
    "        \n",
    "        # Wait for the specific value to be available in the drop-down list\n",
    "        WebDriverWait(self.driver, 10).until(\n",
    "            EC.text_to_be_present_in_element_value((By.ID, 'ctl00_kcMasterPagePlaceHolder_c_YearDropDownList'), current_year)\n",
    "        )\n",
    "        \n",
    "        # Fill out the form (update these selectors based on actual form structure)\n",
    "        self.driver.find_element(By.ID, 'ctl00_kcMasterPagePlaceHolder_c_YearDropDownList').send_keys(current_year)\n",
    "        self.driver.find_element(By.ID, 'ctl00_kcMasterPagePlaceHolder_c_MonthStartDropDownList').send_keys(current_month)\n",
    "        self.driver.find_element(By.ID, 'ctl00_kcMasterPagePlaceHolder_c_EndMonthDropDownList').send_keys(current_month)\n",
    "        # Submit the form\n",
    "        self.driver.find_element(By.ID, 'ctl00_kcMasterPagePlaceHolder_c_DownloadButton').click()\n",
    "    \n",
    "    def cleanup(self):\n",
    "        self.driver.quit()\n",
    "        self.logger.info('Chrome WebDriver quit successfully.')\n",
    "\n",
    "class ProcessData:\n",
    "    \"\"\"\n",
    "    Processes temperature data from CSV files, maintaining all-time high and low temperatures,\n",
    "    as well as high and low temperatures for each year and for each month, collected by lake name.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the ProcessData instance with empty structures for temperature tracking by lake name.\n",
    "        \"\"\"\n",
    "        self.pacific_tz = pytz.timezone('America/Los_Angeles')\n",
    "\n",
    "        self.highs_and_lows = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: {'high': None, 'low': None, 'high_meta': {}, 'low_meta': {}})))\n",
    "        self.monthly_highs_and_lows = defaultdict(lambda: defaultdict(lambda: {'high': None, 'low': None, 'high_meta': {}, 'low_meta': {}}))\n",
    "        self.all_time_high_low = defaultdict(lambda: {'high': None, 'low': None, 'high_meta': {}, 'low_meta': {}})\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def high_and_low_temps(self, csv_file):\n",
    "        \"\"\"\n",
    "        Processes a CSV file to update the temperature records for each year, each month, and all-time, by lake name.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(csv_file, mode='r', encoding='ISO-8859-1') as file:\n",
    "                csv_reader = csv.DictReader(file)\n",
    "                for row in csv_reader:\n",
    "                    #self.logger.info(f'Row: {row}')\n",
    "                    if row['Depth (m)'] and row['Temperature (°C)'] and row['CollectDate']:\n",
    "                        if row['Depth (m)'] != 'Composite' and float(row['Depth (m)']) == 1.0:\n",
    "                            temp = float(row['Temperature (°C)'])\n",
    "                            collect_date = row['CollectDate']\n",
    "                            date_obj = datetime.strptime(collect_date, '%m/%d/%Y')\n",
    "                            year = str(date_obj.year)\n",
    "                            month = str(date_obj.month).zfill(2)  # Ensure month is two digits\n",
    "                            lake_name = row['LakeName']\n",
    "                            monitoring_site = row['MonitoringSite']\n",
    "                            # Update records by lake\n",
    "                            self._update_records(year, month, temp, monitoring_site, collect_date, lake_name)\n",
    "        except FileNotFoundError:\n",
    "            self.logger.error(f\"CSV file {csv_file} not found.\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing CSV file {csv_file}: {e}\")\n",
    "            self.logger.error(f\"Ros: {row}\")\n",
    "            raise\n",
    "\n",
    "    def _update_records(self, year, month, temp, location_name, collect_date, lake_name):\n",
    "        \"\"\"\n",
    "        Updates the temperature records for a specific year, month, and checks for all-time highs and lows, by lake name.\n",
    "        \"\"\"\n",
    "        meta = {'location': location_name, 'lake': lake_name, 'CollectDate': collect_date, 'Year': year}\n",
    "        # Update annual and monthly records by lake\n",
    "        self._update_highs_and_lows(month, temp, meta, self.highs_and_lows[lake_name][year])\n",
    "        # Update monthly records across all years by lake\n",
    "        self._update_highs_and_lows(month, temp, meta, self.monthly_highs_and_lows[lake_name])\n",
    "        # Update all-time high and low by lake\n",
    "        self._update_all_time_high_low(temp, meta, lake_name)\n",
    "    def _update_highs_and_lows(self, month, temp, meta, record):\n",
    "        \"\"\"\n",
    "        Updates high and low temperature records for a specific month within a given record structure, by lake name.\n",
    "        \"\"\"\n",
    "        if record[month]['high'] is None or temp > record[month]['high']:\n",
    "            record[month]['high'] = temp\n",
    "            record[month]['high_meta'] = meta\n",
    "        if record[month]['low'] is None or temp < record[month]['low']:\n",
    "            record[month]['low'] = temp\n",
    "            record[month]['low_meta'] = meta\n",
    "\n",
    "    def _update_all_time_high_low(self, temp, meta, lake_name):\n",
    "        \"\"\"\n",
    "        Updates the all-time high and low temperature records by lake name.\n",
    "        \"\"\"\n",
    "        record = self.all_time_high_low[lake_name]\n",
    "        if record['high'] is None or temp > record['high']:\n",
    "            record['high'] = temp\n",
    "            record['high_meta'] = meta\n",
    "        if record['low'] is None or temp < record['low']:\n",
    "            record['low'] = temp\n",
    "            record['low_meta'] = meta\n",
    "\n",
    "    def print_records(self):\n",
    "        \"\"\"\n",
    "        Prints the temperature records for each year, each month, and all-time, by lake name.\n",
    "        \"\"\"\n",
    "        for lake, records in self.highs_and_lows.items():\n",
    "            self.logger.info(f\"Lake: {lake}\")\n",
    "            self.logger.info(f\"All-time High: {self.all_time_high_low[lake]['high']} Location: {self.all_time_high_low[lake]['high_meta']['location']} Collect Date: {self.all_time_high_low[lake]['high_meta']['CollectDate']}\")\n",
    "            self.logger.info(f\"All-time Low: {self.all_time_high_low[lake]['low']} Location: {self.all_time_high_low[lake]['low_meta']['location']} Collect Date: {self.all_time_high_low[lake]['low_meta']['CollectDate']}\")\n",
    "            self.logger.info(f\"Monthly Highs and Lows:\")\n",
    "            for month, temps in self.monthly_highs_and_lows[lake].items():\n",
    "                self.logger.info(f\"Month: {month} - High: {temps['high']} - Low: {temps['low']} Location: {temps['high_meta']['location']} Collect Date: {temps['high_meta']['CollectDate']}\")\n",
    "\n",
    "    def celsius_to_fahrenheit(self, celsius):\n",
    "        \"\"\"\n",
    "        Converts a temperature from Celsius to Fahrenheit.\n",
    "        \n",
    "        Parameters:\n",
    "        - celsius (float): Temperature in Celsius.\n",
    "        \n",
    "        Returns:\n",
    "        - float: Temperature in Fahrenheit.\n",
    "        \"\"\"\n",
    "        return (celsius * 9/5) + 32\n",
    "\n",
    "    def get_highs_and_lows_json(self):\n",
    "        \"\"\"\n",
    "        Returns the temperature records in JSON format, including temperatures in Fahrenheit.\n",
    "        \n",
    "        Returns:\n",
    "        - str: JSON string containing the high and low temperature records.\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        data['last_updated'] = datetime.now(self.pacific_tz).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        for lake, records in self.highs_and_lows.items():\n",
    "            lake_data = {\n",
    "                'all_time_high': self.all_time_high_low[lake]['high'],\n",
    "                'all_time_low': self.all_time_high_low[lake]['low'],\n",
    "                'all_time_high_fahrenheit': self.celsius_to_fahrenheit(self.all_time_high_low[lake]['high']) if self.all_time_high_low[lake]['high'] is not None else None,\n",
    "                'all_time_low_fahrenheit': self.celsius_to_fahrenheit(self.all_time_high_low[lake]['low']) if self.all_time_high_low[lake]['low'] is not None else None,\n",
    "                'all_time_high_meta': self.all_time_high_low[lake]['high_meta'],\n",
    "                'all_time_low_meta': self.all_time_high_low[lake]['low_meta'],\n",
    "                'monthly_highs_and_lows': {}\n",
    "            }\n",
    "            for month, temps in self.monthly_highs_and_lows[lake].items():\n",
    "                lake_data['monthly_highs_and_lows'][month] = {\n",
    "                    'high': temps['high'],\n",
    "                    'low': temps['low'],\n",
    "                    'high_fahrenheit': self.celsius_to_fahrenheit(temps['high']) if temps['high'] is not None else None,\n",
    "                    'low_fahrenheit': self.celsius_to_fahrenheit(temps['low']) if temps['low'] is not None else None,\n",
    "                    'high_meta': temps['high_meta'],\n",
    "                    'low_meta': temps['low_meta']\n",
    "                }\n",
    "            data[lake] = lake_data\n",
    "        \n",
    "        # Convert the data dictionary to a JSON string\n",
    "        return json.dumps(data, sort_keys=True,indent=4)\n",
    "\n",
    "class KingCountyLakes():\n",
    "    def __init__(self):\n",
    "        self.pacific_tz = pytz.timezone('America/Los_Angeles')\n",
    "        self.file_manager = FileManager(use_s3=False)\n",
    "        self.s3_file_manager = FileManager(use_s3=True)\n",
    "        self.chrome_helper = ChromeUtility(headless=True,download_directory=self.file_manager.download_directory)\n",
    "        self.data_processor = ProcessData()\n",
    "        self.lake_temps_json_file = f'lake_temps.json'\n",
    "        self.highs_and_lows_json_file = f'lake_wa_highs_and_lows.json'\n",
    "        self.king_county_real_time_url = 'https://green2.kingcounty.gov/lake-buoy/GenerateMapData.aspx'\n",
    "        self.highs_and_lows_prod_url = 'https://swimming.withelvis.com/WA/lake_wa_highs_and_lows.json'\n",
    "        self.lake_temps_prod_url = 'https://swimming.withelvis.com/WA/lake_temps.json'\n",
    "        self.prod_url_dict = {\n",
    "            'highs and lows': { \n",
    "                'url': self.highs_and_lows_prod_url,\n",
    "                'hours_to_cache': 1440, # 60 days = 1440\n",
    "                'update': False\n",
    "            },\n",
    "            'lake temps': { \n",
    "                'url': self.lake_temps_prod_url,\n",
    "                'hours_to_cache': 8,\n",
    "                'update': False\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def get_all_historical_wa_lake_data(self):\n",
    "        locations = self.chrome_helper.get_historical_wa_lake_data_locations()\n",
    "\n",
    "        for location in locations:\n",
    "            self.file_manager.logger.info(f'Location: {location}')\n",
    "            new_file_directory = f'{location}' if self.file_manager.use_s3 else f'{self.file_manager.new_file_base}/{location}'\n",
    "            self.file_manager.logger.info(f'New file directory: {new_file_directory}')\n",
    "            self.file_manager.mkdirs(new_file_directory)\n",
    "            new_file_path = f'{new_file_directory}/{location}.csv'\n",
    "            backup_file_path = f'{new_file_path}.{self.file_manager.backup_file_extension}'\n",
    "            self.file_manager.logger.info(f'new file path: {new_file_path}')\n",
    "            self.file_manager.logger.info(f'backup_file_path: {backup_file_path}')\n",
    "            self.chrome_helper.get_historical_wa_lake_data(location=location)\n",
    "            file_path = self.file_manager.wait_for_download_to_complete(timeout=30)\n",
    "            self.file_manager.logger.info(f'File path: {file_path}')\n",
    "            self.file_manager.backup_file(new_file_path,backup_file_path)\n",
    "            self.file_manager.move_file(file_path,new_file_path)\n",
    "\n",
    "    def get_combined_csv_file_path(self):\n",
    "        combined_csv_directory = f'{self.file_manager.new_file_base}/WA'\n",
    "        self.file_manager.mkdirs(combined_csv_directory)\n",
    "        new_combined_csv_file_path = f'{combined_csv_directory}/all_historical_wa_lake_data.csv'\n",
    "        return new_combined_csv_file_path\n",
    "\n",
    "    def combine_historical_csv(self):\n",
    "        if self.chrome_helper.historical_locations == '':\n",
    "            self.file_manager.logger.info('No historical locations found. Getting historical locations.')\n",
    "            self.chrome_helper.get_historical_wa_lake_data_locations()\n",
    "        new_combined_csv_file_path = self.get_combined_csv_file_path()\n",
    "        backup_combined_csv_file_path = f'{new_combined_csv_file_path}.{self.file_manager.backup_file_extension}'\n",
    "        self.file_manager.backup_file(new_combined_csv_file_path,backup_combined_csv_file_path)\n",
    "        for location in self.chrome_helper.historical_locations:\n",
    "            file_to_append = f'{self.file_manager.download_directory}/{location}/{location}.csv'\n",
    "            self.file_manager.logger.info(f'Calling file_manager.combine_csv for {location} for file {file_to_append}' )\n",
    "            self.file_manager.combine_csv(file_to_append,new_combined_csv_file_path)\n",
    "        return new_combined_csv_file_path\n",
    "\n",
    "    def get_highs_and_lows(self,combined_csv_file):\n",
    "        self.data_processor.high_and_low_temps(combined_csv_file)\n",
    "        return self.data_processor.get_highs_and_lows_json()\n",
    "\n",
    "    def write_highs_and_lows_to_json_file(self):\n",
    "        new_combined_csv_file_path = self.get_combined_csv_file_path()\n",
    "        if not os.path.isfile(new_combined_csv_file_path):\n",
    "            new_combined_csv_file_path = self.combine_historical_csv()    \n",
    "        highs_and_lows_json_file_path = f'{self.file_manager.download_directory}/WA/{self.highs_and_lows_json_file}'\n",
    "        highs_and_lows_backup_file_path = f'{highs_and_lows_json_file_path}.{self.file_manager.backup_file_extension}'\n",
    "        highs_and_lows_json = self.get_highs_and_lows(new_combined_csv_file_path)\n",
    "        # create a backup of the file if it already exists\n",
    "        self.file_manager.backup_file(highs_and_lows_json_file_path,highs_and_lows_backup_file_path)\n",
    "        # write the json object to the file\n",
    "        with open(highs_and_lows_json_file_path, 'w') as file:\n",
    "            file.write(highs_and_lows_json)\n",
    "        return highs_and_lows_json_file_path\n",
    "\n",
    "    def upload_highs_and_lows_to_s3(self,highs_and_lows_json_file_path):\n",
    "        self.s3_file_manager.mkdirs('WA')\n",
    "        s3_json_file_path = f'WA/{self.highs_and_lows_json_file}'\n",
    "        s3_backup_json_file_path = f'{s3_json_file_path}.{self.file_manager.backup_file_extension}'\n",
    "        self.s3_file_manager.backup_file(s3_json_file_path,s3_backup_json_file_path)\n",
    "        self.s3_file_manager.move_file(highs_and_lows_json_file_path,f'WA/{self.highs_and_lows_json_file}',remove_source=False)\n",
    "\n",
    "    def process_all_historical(self):\n",
    "        self.get_all_historical_wa_lake_data()\n",
    "        self.combine_historical_csv()\n",
    "        highs_and_lows_json_file_path = self.write_highs_and_lows_to_json_file()\n",
    "        self.upload_highs_and_lows_to_s3(highs_and_lows_json_file_path)\n",
    "\n",
    "    def invalidate_cloudfront_cache(self):\n",
    "        self.file_manager.logger.info('Invalidating CloudFront cache')\n",
    "        client = boto3.client('cloudfront')\n",
    "        response = client.create_invalidation(\n",
    "        DistributionId='ESWMI6WOZH1YK',  # replace with your distribution ID\n",
    "        InvalidationBatch={\n",
    "            'Paths': {\n",
    "                'Quantity': 1,\n",
    "                'Items': [\n",
    "                    '/*',  # this will invalidate all objects\n",
    "                ]\n",
    "            },\n",
    "            'CallerReference': str(time.time())  # unique identifier for this invalidation request\n",
    "        }\n",
    "        )\n",
    "    \n",
    "    def get_cached_realtime_lake_data(self):\n",
    "        locations = self.chrome_helper.get_cached_realtime_wa_lake_data_locations()\n",
    "        new_files = []\n",
    "        for location in locations:\n",
    "            lake_name = f\"lake_{location.split(' ')[0].lower()}\"\n",
    "            self.file_manager.logger.info(f'Location: {lake_name}')\n",
    "            new_file_directory = f'{lake_name}' if self.file_manager.use_s3 else f'{self.file_manager.new_file_base}/{lake_name}'\n",
    "            self.file_manager.logger.info(f'New file directory: {new_file_directory}')\n",
    "            self.file_manager.mkdirs(new_file_directory)\n",
    "            new_file_path = f'{new_file_directory}/{lake_name}.txt'\n",
    "            backup_file_path = f'{new_file_path}.{self.file_manager.backup_file_extension}'\n",
    "            self.file_manager.logger.info(f'new file path: {new_file_path}')\n",
    "            self.file_manager.logger.info(f'backup_file_path: {backup_file_path}')\n",
    "            self.chrome_helper.get_realtime_wa_lake_data(location=location)\n",
    "            file_path = self.file_manager.wait_for_download_to_complete(timeout=30)\n",
    "            self.file_manager.logger.info(f'File path: {file_path}')\n",
    "            self.file_manager.backup_file(new_file_path,backup_file_path)\n",
    "            self.file_manager.move_file(file_path,new_file_path)\n",
    "            new_files.append(new_file_path)\n",
    "        return new_files\n",
    "        \n",
    "    def process_cached_realtime_lake_data(self,file):\n",
    "        self.file_manager.logger.info(f'Processing file {file}')\n",
    "        # Load the data\n",
    "        df = pd.read_csv(file,delimiter='\\t',)\n",
    "        # Make sure column names are correctly identified\n",
    "        df.columns = [col.strip() for col in df.columns]\n",
    "        \n",
    "        # Round the \"Depth (m)\" to the closest integer\n",
    "        df['Depth (m)'] = df['Depth (m)'].round().astype(int)\n",
    "        \n",
    "        # Filter for depth of 1 and provisional column value is \"no\"\n",
    "        filtered_df = df[(df['Depth (m)'] == 1)]\n",
    "        \n",
    "        # Find the most recent temperature\n",
    "        df.loc[filtered_df.index, 'Date'] = pd.to_datetime(filtered_df['Date'],format='%m/%d/%Y %I:%M:%S %p')\n",
    "        \n",
    "        most_recent_row = filtered_df.sort_values('Date', ascending=False).iloc[0]\n",
    "        \n",
    "        # Convert temperature to Fahrenheit\n",
    "        temp_celsius = most_recent_row['Temperature (°C)']\n",
    "        temp_fahrenheit = self.data_processor.celsius_to_fahrenheit(celsius=temp_celsius)\n",
    "\n",
    "        # Construct the JSON object\n",
    "        \n",
    "        json_object = {\n",
    "            'most_recent_temperature_celsius': temp_celsius,\n",
    "            'most_recent_temperature_fahrenheit': temp_fahrenheit,\n",
    "            'date_of_collection': most_recent_row['Date'] #.strftime('%Y-%m-%d')  # Format the date\n",
    "        }\n",
    "        return json_object\n",
    "\n",
    "    def save_cached_realtime_lake_data(self,files):\n",
    "        data = {}\n",
    "        data['last_updated'] = datetime.now(self.pacific_tz).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        lake_name = ''\n",
    "        for f in files:\n",
    "            lake_name = f.split('/')[-1].split('.')[0]  # Extract file prefix\n",
    "            lake_data = self.process_cached_realtime_lake_data(f)\n",
    "            data[lake_name] = lake_data\n",
    "        self.write_realtime_lake_data(data)\n",
    "\n",
    "    def write_realtime_lake_data(self,json_object):\n",
    "        self.file_manager.logger.info(f'Real-time json temps object: {json.dumps(json_object,indent=4)}')\n",
    "        file_name = self.lake_temps_json_file\n",
    "        new_file_directory = f'{self.file_manager.new_file_base}/WA/'\n",
    "        new_s3_file_directory = f'WA'\n",
    "        self.file_manager.logger.info(f'New local file directory: {new_file_directory}')\n",
    "        self.file_manager.mkdirs(new_file_directory)\n",
    "        self.s3_file_manager.mkdirs(new_file_directory)\n",
    "        new_file_path = f'{new_file_directory}/{file_name}'\n",
    "        backup_file_path = f'{new_file_path}.{self.file_manager.backup_file_extension}'\n",
    "        self.file_manager.logger.info(f'New file path: {new_file_path}')\n",
    "        self.file_manager.logger.info(f'Backup file path: {backup_file_path}')\n",
    "        self.file_manager.backup_file(new_file_path,backup_file_path)\n",
    "        new_s3_file_path = f'{new_s3_file_directory}/{file_name}'\n",
    "        backup_s3_file_path = f'{new_s3_file_path}.{self.file_manager.backup_file_extension}'\n",
    "        self.file_manager.logger.info(f'New s3 file path: {new_s3_file_path}')\n",
    "        self.file_manager.logger.info(f'Backup s3 file path: {backup_s3_file_path}')\n",
    "        self.s3_file_manager.backup_file(new_s3_file_path,backup_s3_file_path)\n",
    "        # write the json object to the file\n",
    "        with open(new_file_path, 'w') as file:\n",
    "            file.write(json.dumps(json_object,indent=4))\n",
    "        self.file_manager.logger.info(f'wrote file: {new_file_path}')\n",
    "        self.s3_file_manager.move_file(new_file_path,new_s3_file_path,remove_source=False)\n",
    "\n",
    "    def get_realtime_lake_data(self):\n",
    "        url = self.king_county_real_time_url \n",
    "        self.file_manager.logger.info(f'Getting realtime lake data from {url}')\n",
    "        get_cached = False\n",
    "        response = None\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "        except Exception as e:\n",
    "            self.file_manager.logger.info(f'Exception thrown accessing {url}.')\n",
    "            get_cached = True\n",
    "        # check if the response contains the expected keys\n",
    "        if get_cached == False and response.text.find('|Washington|') == -1 and response.text.find('|Sammamish|') == -1:\n",
    "            self.file_manager.logger.error(f'Failed to get data from {url} - keys not found.')\n",
    "            get_cached = True\n",
    "        if get_cached:\n",
    "            self. file_manager.logger.error(f'Failed to get data from {url}. Retrieving cached data...')\n",
    "            files = self.get_cached_realtime_lake_data()\n",
    "            #files = ['/Users/jeffmalek/Documents/git/lake-washington-and-sammamish/downloads/lake_sammamish/lake_sammamish.txt','/Users/jeffmalek/Documents/git/lake-washington-and-sammamish/downloads/lake_washington/lake_washington.txt']\n",
    "            self.save_cached_realtime_lake_data(files)  \n",
    "            return\n",
    "        self.file_manager.logger.info(f'Got realtime lake data from {url}')\n",
    "        # Split the response into segments\n",
    "        segments = re.split('N\\^|Y\\^', response.text)\n",
    "        lake_segments = [next((segment for segment in segments if lake_name in segment), None) for lake_name in ['Washington', 'Sammamish']]\n",
    "        data = {}\n",
    "        data['last_updated'] = datetime.now(self.pacific_tz).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        for segment in lake_segments:\n",
    "            # Split the segment into fields\n",
    "            fields = segment.split('|')\n",
    "            lake_name = f'lake_{fields[1].lower().strip()}'\n",
    "            temperature_celsius = float(fields[6].strip())\n",
    "            self.file_manager.logger.info(f'{segment} temp: {temperature_celsius}')\n",
    "            temperature_fahrenheit = self.data_processor.celsius_to_fahrenheit(float(temperature_celsius))\n",
    "            date_of_collection = fields[2].strip()\n",
    "            # Create a JSON object with the required fields\n",
    "            lake_data = {\n",
    "                'most_recent_temperature_celsius': temperature_celsius,\n",
    "                'most_recent_temperature_fahrenheit': temperature_fahrenheit,\n",
    "                'date_of_collection': date_of_collection   \n",
    "            }\n",
    "            data[lake_name] = lake_data   \n",
    "        self.write_realtime_lake_data(data)  \n",
    "\n",
    "    def get_latest_production_json(self):\n",
    "        keys = list(self.prod_url_dict.keys())\n",
    "        for key in keys:\n",
    "            url = self.prod_url_dict[key][\"url\"]\n",
    "            hours_to_cache = self.prod_url_dict[key][\"hours_to_cache\"]\n",
    "            self.file_manager.logger.info(f'Getting latest production json for {key} from {url} with {hours_to_cache} days to cache.')\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "            except Exception as e:\n",
    "                self.file_manager.logger.error(f'Failed to get latest production json from {url}')\n",
    "                raise\n",
    "            json_response = response.json()\n",
    "            server_time = response.headers.get('Date', None)\n",
    "            last_modified = response.headers.get('Last-Modified', None)\n",
    "            age = response.headers.get('Age', None) \n",
    "            self.file_manager.logger.info(f'Response server time: {server_time} Last modified: {last_modified} Age in seconds: {age}')\n",
    "            # calculate whether it's time to request new data\n",
    "            last_updated = json_response.get('last_updated',None)\n",
    "            formatted_last_updated = datetime.strptime(last_updated, \"%Y-%m-%d %H:%M:%S\")\n",
    "            formatted_last_updated = self.pacific_tz.localize(formatted_last_updated)\n",
    "            time_diff = datetime.now(self.pacific_tz) - formatted_last_updated\n",
    "            hours_diff = time_diff.total_seconds() / 3600\n",
    "            if hours_diff > hours_to_cache:\n",
    "                self.prod_url_dict[key][\"update\"] = True\n",
    "            self.file_manager.logger.info(f'Data for {key} Last updated: {last_updated} time diff: {time_diff} hours diff: {hours_diff} hours to cache: {hours_to_cache}')\n",
    "        \n",
    "    def go(self):\n",
    "        self.get_latest_production_json()\n",
    "        self.file_manager.logger.info(f'prod_url_dict: {json.dumps(self.prod_url_dict,indent=4)}')\n",
    "        if self.prod_url_dict['highs and lows'][\"update\"]:\n",
    "            self.file_manager.logger.info(f'+++++++++ Processing all historical data...')\n",
    "            self.process_all_historical()\n",
    "            self.invalidate_cloudfront_cache()\n",
    "        else:\n",
    "            self.file_manager.logger.info(f'+++++++++ Not time to process all historical data.')\n",
    "\n",
    "        if self.prod_url_dict['lake temps'][\"update\"]:\n",
    "            self.file_manager.logger.info(f'+++++++++ Processing real-time lake data...')\n",
    "            self.get_realtime_lake_data()\n",
    "            self.invalidate_cloudfront_cache()\n",
    "        else:\n",
    "            self.file_manager.logger.info(f'+++++++++ Not time to process real-time lake data.')\n",
    "        \n",
    "        \n",
    "    def __del__(self):\n",
    "        self.chrome_helper.cleanup()\n",
    "\n",
    "def main():\n",
    "    king_county_lakes = KingCountyLakes()\n",
    "    king_county_lakes.go()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# to gather Lake WA and Sammamish temps and push to S3.\n",
    "# king_county_lakes.get_realtime_lake_data()\n",
    "\n",
    "# to process all lake historical data and upload the high-low temps to S3.\n",
    "# king_county_lakes.process_all_historical()\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
